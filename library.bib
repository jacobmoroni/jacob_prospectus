Automatically generated by Mendeley Desktop 1.18
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Henry2010,
abstract = {RGB-D cameras are novel sensing systems that capture RGB images along with per-pixel depth information. RGB-D cameras rely on either structured light patterns combined with stereo sensing [6, 10] or time-of-flight laser sensing [1] to generate depth estimates that can be associated with RGB pixels. Very soon, small, high-quality RGB-D cameras developed for computer gaming and home entertainment applications will become available at cost below {\$}100. In this paper we investigate how such cameras can be used in the context of robotics, specifically for building dense 3D maps of indoor environments. Such maps have applications in robot navigation, manipulation, semantic mapping, and telepresence. The robotics and computer vision communities have developed a variety of techniques for 3D mapping based on laser range scans [8, 11], stereo cameras [7], monocular cameras [3], and unsorted collections of photos [4]. While RGB-D cameras provide the opportunity to build 3D maps of unprecedented richness, they have drawbacks that make their application to 3D mapping difficult: They provide depth only up to a limited distance (typically less than 5m), depth values are much noisier than those provided by laser scanners, and their field of view (∼ 60 •) is far more constrained than that of specialized cameras or laser scanners typically used for 3D mapping (∼ 180 •). In our work, we use a camera developed by PrimeSense [10]. The key insights of this investigation are: first, that existing frame matching techniques are not sufficient to provide robust visual odometry with these cameras; second, that a tight integration of depth and color information can yield robust frame matching and loop closure detection; third, that building on best practice techniques in SLAM and computer graphics makes it possible to build and visualize accurate and extremely rich 3D maps with such cameras; and, fourth, that it will be feasible to build complete robot navigation and interaction systems solely based on cheap depth cameras. 2 Technical Approach Following best practice in robot mapping, our RGB-D mapping technique consists of three key components: first, the spatial alignment of consecutive data frames; second, the detection of loop closures; and, third, the globally consistent alignment of the complete data sequence. Alignment between successive frames is computed by jointly optimizing over both appearance and shape matching. Appearance-based alignment is done with RANSAC over SIFT features annotated with 3D position. The 3D SIFT matching requires no initial estimate of the relative pose, but can fail if a frame contains few distinctive visual feature points. Shape-based alignment is performed through ICP using a point-to-plane error metric [2]. ICP alignment requires a good initial estimate of the relative pose, but allows the full 3D shape of the data to constrain alignment. In our joint optimization framework, we run 3D SIFT to obtain an initial alignment, and then the inliers from the RANSAC solution are included as fixed point-to-point constraints alongside the point-to-plane constraints from the full point cloud. In this way our system can handle situations in which only RGB or shape alone would fail to generate good alignments. Our approach detects loop closures by matching data frames against a subset of previously collected frames using 3D SIFT. To generate globally consistent alignments we use TORO, a pose-graph optimization tool developed for robotics SLAM [5]. The overall system can accurately align and map large indoor environments in near real time and is capable of handling extreme situations such as featureless corridors and completely dark rooms. Once we have aligned the frames, we build a global map in the form of small colored surface patches called surfels [9]. This representation enables efficient reasoning about occlusions and color for each part of the environment, and provides good visualizations of the resulting model. Furthermore, surfels automatically adapt the resolution of the representation to the quality and resolution of data available for each patch. A closeup of the surfel representation can be seen in Fig. 1(c).},
author = {Henry, Peter and Krainin, Michael and Herbst, Evan and Ren, Xiaofeng and Fox, Dieter},
doi = {10.1007/978-3-642-28572-1_33},
file = {:home/jacob/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henry et al. - Unknown - RGB-D Mapping Using Depth Cameras for Dense 3D Modeling of Indoor Environments.pdf:pdf},
isbn = {978-3-642-28571-4},
issn = {0278-3649},
journal = {{12th International Symposium on Experimental Robotics (ISER{\}}},
pages = {1},
title = {{RGB-DMapping: Using Depth Cameras for Dense 3D Modeling of Indoor Environments}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.226.91{\&}rep=rep1{\&}type=pdf http://link.springer.com/10.1007/978-3-642-28572-1},
year = {2010}
}
@article{Henry2010a,
abstract = {RGB-D cameras are novel sensing systems that capture RGB images along with per-pixel depth information. RGB-D cameras rely on either structured light patterns combined with stereo sensing [6, 10] or time-of-flight laser sensing [1] to generate depth estimates that can be associated with RGB pixels. Very soon, small, high-quality RGB-D cameras developed for computer gaming and home entertainment applications will become available at cost below {\$}100. In this paper we investigate how such cameras can be used in the context of robotics, specifically for building dense 3D maps of indoor environments. Such maps have applications in robot navigation, manipulation, semantic mapping, and telepresence. The robotics and computer vision communities have developed a variety of techniques for 3D mapping based on laser range scans [8, 11], stereo cameras [7], monocular cameras [3], and unsorted collections of photos [4]. While RGB-D cameras provide the opportunity to build 3D maps of unprecedented richness, they have drawbacks that make their application to 3D mapping difficult: They provide depth only up to a limited distance (typically less than 5m), depth values are much noisier than those provided by laser scanners, and their field of view (∼ 60 •) is far more constrained than that of specialized cameras or laser scanners typically used for 3D mapping (∼ 180 •). In our work, we use a camera developed by PrimeSense [10]. The key insights of this investigation are: first, that existing frame matching techniques are not sufficient to provide robust visual odometry with these cameras; second, that a tight integration of depth and color information can yield robust frame matching and loop closure detection; third, that building on best practice techniques in SLAM and computer graphics makes it possible to build and visualize accurate and extremely rich 3D maps with such cameras; and, fourth, that it will be feasible to build complete robot navigation and interaction systems solely based on cheap depth cameras. 2 Technical Approach Following best practice in robot mapping, our RGB-D mapping technique consists of three key components: first, the spatial alignment of consecutive data frames; second, the detection of loop closures; and, third, the globally consistent alignment of the complete data sequence. Alignment between successive frames is computed by jointly optimizing over both appearance and shape matching. Appearance-based alignment is done with RANSAC over SIFT features annotated with 3D position. The 3D SIFT matching requires no initial estimate of the relative pose, but can fail if a frame contains few distinctive visual feature points. Shape-based alignment is performed through ICP using a point-to-plane error metric [2]. ICP alignment requires a good initial estimate of the relative pose, but allows the full 3D shape of the data to constrain alignment. In our joint optimization framework, we run 3D SIFT to obtain an initial alignment, and then the inliers from the RANSAC solution are included as fixed point-to-point constraints alongside the point-to-plane constraints from the full point cloud. In this way our system can handle situations in which only RGB or shape alone would fail to generate good alignments. Our approach detects loop closures by matching data frames against a subset of previously collected frames using 3D SIFT. To generate globally consistent alignments we use TORO, a pose-graph optimization tool developed for robotics SLAM [5]. The overall system can accurately align and map large indoor environments in near real time and is capable of handling extreme situations such as featureless corridors and completely dark rooms. Once we have aligned the frames, we build a global map in the form of small colored surface patches called surfels [9]. This representation enables efficient reasoning about occlusions and color for each part of the environment, and provides good visualizations of the resulting model. Furthermore, surfels automatically adapt the resolution of the representation to the quality and resolution of data available for each patch. A closeup of the surfel representation can be seen in Fig. 1(c).},
author = {Henry, Peter and Krainin, Michael and Herbst, Evan and Ren, Xiaofeng and Fox, Dieter},
doi = {10.1007/978-3-642-28572-1_33},
file = {:home/jacob/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henry et al. - Unknown - RGB-D Mapping Using Depth Cameras for Dense 3D Modeling of Indoor Environments.pdf:pdf},
isbn = {978-3-642-28571-4},
issn = {0278-3649},
journal = {{12th International Symposium on Experimental Robotics (ISER{\}}},
pages = {1},
title = {{RGB-DMapping: Using Depth Cameras for Dense 3D Modeling of Indoor Environments}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.226.91{\&}rep=rep1{\&}type=pdf http://link.springer.com/10.1007/978-3-642-28572-1},
year = {2010}
}
@article{Ellekilde2007,
author = {Ellekilde, Lars-Peter and Huang, Shoudong and {Valls Mir{\'{o}}}, Jaime and Dissanayake, Gamini},
doi = {10.1002/rob.20173},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {jan},
number = {1-2},
pages = {71--89},
title = {{Dense 3D Map Construction for Indoor Search and Rescue}},
url = {http://doi.wiley.com/10.1002/rob.20173},
volume = {24},
year = {2007}
}
@article{Labbe2013,
abstract = {In appearance-based localization and mapping, loop- closure detection is the process used to determinate if the cur- rent observation comes from a previously visited location or a new one. As the size of the internal map increases, so does the time required to compare new observations with all stored loca- tions, eventually limiting online processing. This paper presents an online loop-closure detection approach for large-scale and long- term operation. The approach is based on amemorymanagement method,which limits the number of locations used for loop-closure detection so that the computation time remains under real-time constraints. The idea consists of keeping the most recent and fre- quently observed locations in a working memory (WM) that is used for loop-closure detection, and transferring the others into a long-term memory (LTM). When a match is found between the current location and one stored in WM, associated locations that are stored in LTM can be updated and remembered for addi- tional loop-closure detections. Results demonstrate the approach's adaptability and scalability using ten standard datasets from other appearance-based loop-closure approaches, one custom dataset us- ing real images taken over a 2-km loop of our university campus, and one custom dataset (7 h) using virtual images from the racing video game “Need for Speed:MostWanted.”},
author = {Labbe, Mathieu and Michaud, Francois},
doi = {10.1109/TRO.2013.2242375},
file = {:home/jacob/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Labb{\'{e}}, Michaud - Unknown - Appearance-Based Loop Closure Detection for Online Large-Scale and Long-Term Operation.pdf:pdf},
isbn = {978-1-61284-455-8},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Appearance-based localization and mapping,bag-of-words approach,dynamic Bayes filtering,place recognition},
number = {3},
pages = {734--745},
pmid = {6459608},
title = {{Appearance-based loop closure detection for online large-scale and long-term operation}},
url = {https://introlab.3it.usherbrooke.ca/mediawiki-introlab/images/b/bc/TRO2013.pdf},
volume = {29},
year = {2013}
}
@misc{Luotsinen2004,
author = {Luotsinen, Linus J and Gonzalez, Avelino J and Boeloeni, Ladislau},
file = {:home/jacob/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luotsinen, Gonzalez, Boeloeni - 2004 - Collaborative UAV Exploration of Hostile Environments.pdf:pdf},
keywords = {*ALGORITHMS,*AUTONOMOUS NAVIGATION,*COMBAT AREAS,*SURVEILLANCE DRONES,*TACTICAL DATA SYSTEMS,ADVERSE CONDITIONS,ARTIFICIAL INTELLIGENCE,COLLABORATIVE TECHNIQUES,COMPUTERIZED SIMULATION,DECISION MAKING,GRIDS(COORDINATES),INFORMATION TRANSFER,MAPS,SYMPOSIA},
title = {{Collaborative UAV Exploration of Hostile Environments}},
url = {http://www.dtic.mil/docs/citations/ADA432922},
year = {2004}
}
@inproceedings{Labbe2011a,
author = {Labbe, M. and Michaud, F.},
booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6094602},
file = {:home/jacob/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Labb{\'{e}}, Michaud - Unknown - Memory Management for Real-Time Appearance-Based Loop Closure Detection.pdf:pdf},
isbn = {978-1-61284-456-5},
month = {sep},
pages = {1271--1276},
publisher = {IEEE},
title = {{Memory management for real-time appearance-based loop closure detection}},
url = {http://ieeexplore.ieee.org/document/6094602/},
year = {2011}
}
@article{Wheeler2017,
author = {Wheeler, David and Koch, Daniel and Jackson, James and Ellingson, Gary and Nyholm, Paul and McLain, Timothy and Beard, Randal},
journal = {All Faculty Publications},
month = {aug},
title = {{Relative Navigation of Autonomous GPS-Degraded Micro Air Vehicles}},
url = {https://scholarsarchive.byu.edu/facpub/1962},
year = {2017}
}
@inproceedings{Bryson2007,
author = {Bryson, Mitch and Sukkarieh, Salah},
booktitle = {2007 IEEE Aerospace Conference},
doi = {10.1109/AERO.2007.352850},
isbn = {1-4244-0524-6},
pages = {1--12},
publisher = {IEEE},
title = {{Co-operative Localisation and Mapping for Multiple UAVs in Unknown Environments}},
url = {http://ieeexplore.ieee.org/document/4161346/},
year = {2007}
}
@article{Hornung2013,
author = {Hornung, Armin and Wurm, Kai M. and Bennewitz, Maren and Stachniss, Cyrill and Burgard, Wolfram},
doi = {10.1007/s10514-012-9321-0},
file = {:home/jacob/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornung et al. - 2013 - OctoMap an efficient probabilistic 3D mapping framework based on octrees.pdf:pdf},
issn = {0929-5593},
journal = {Autonomous Robots},
month = {apr},
number = {3},
pages = {189--206},
publisher = {Springer US},
title = {{OctoMap: an efficient probabilistic 3D mapping framework based on octrees}},
url = {http://link.springer.com/10.1007/s10514-012-9321-0},
volume = {34},
year = {2013}
}
@article{Michael2012,
author = {Michael, Nathan and Shen, Shaojie and Mohta, Kartik and Mulgaonkar, Yash and Kumar, Vijay and Nagatani, Keiji and Okada, Yoshito and Kiribayashi, Seiga and Otake, Kazuki and Yoshida, Kazuya and Ohno, Kazunori and Takeuchi, Eijiro and Tadokoro, Satoshi},
doi = {10.1002/rob.21436},
file = {:home/jacob/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Michael et al. - 2012 - Collaborative mapping of an earthquake-damaged building via ground and aerial robots.pdf:pdf},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {sep},
number = {5},
pages = {832--841},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
title = {{Collaborative mapping of an earthquake-damaged building via ground and aerial robots}},
url = {http://doi.wiley.com/10.1002/rob.21436},
volume = {29},
year = {2012}
}
@inproceedings{Labbe2011a,
author = {Labbe, M. and Michaud, F.},
booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6094602},
file = {:home/jacob/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Labb{\'{e}}, Michaud - Unknown - Memory Management for Real-Time Appearance-Based Loop Closure Detection.pdf:pdf},
isbn = {978-1-61284-456-5},
month = {sep},
pages = {1271--1276},
publisher = {IEEE},
title = {{Memory management for real-time appearance-based loop closure detection}},
url = {http://ieeexplore.ieee.org/document/6094602/},
year = {2011}
}
@article{Michael2012,
author = {Michael, Nathan and Shen, Shaojie and Mohta, Kartik and Mulgaonkar, Yash and Kumar, Vijay and Nagatani, Keiji and Okada, Yoshito and Kiribayashi, Seiga and Otake, Kazuki and Yoshida, Kazuya and Ohno, Kazunori and Takeuchi, Eijiro and Tadokoro, Satoshi},
doi = {10.1002/rob.21436},
file = {:home/jacob/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Michael et al. - 2012 - Collaborative mapping of an earthquake-damaged building via ground and aerial robots.pdf:pdf},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {sep},
number = {5},
pages = {832--841},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
title = {{Collaborative mapping of an earthquake-damaged building via ground and aerial robots}},
url = {http://doi.wiley.com/10.1002/rob.21436},
volume = {29},
year = {2012}
}
@inproceedings{Bryson2007,
author = {Bryson, Mitch and Sukkarieh, Salah},
booktitle = {2007 IEEE Aerospace Conference},
doi = {10.1109/AERO.2007.352850},
isbn = {1-4244-0524-6},
pages = {1--12},
publisher = {IEEE},
title = {{Co-operative Localisation and Mapping for Multiple UAVs in Unknown Environments}},
url = {http://ieeexplore.ieee.org/document/4161346/},
year = {2007}
}
@article{Ellekilde2007,
author = {Ellekilde, Lars-Peter and Huang, Shoudong and {Valls Mir{\'{o}}}, Jaime and Dissanayake, Gamini},
doi = {10.1002/rob.20173},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {jan},
number = {1-2},
pages = {71--89},
title = {{Dense 3D Map Construction for Indoor Search and Rescue}},
url = {http://doi.wiley.com/10.1002/rob.20173},
volume = {24},
year = {2007}
}
@article{Labbe2013,
abstract = {In appearance-based localization and mapping, loop- closure detection is the process used to determinate if the cur- rent observation comes from a previously visited location or a new one. As the size of the internal map increases, so does the time required to compare new observations with all stored loca- tions, eventually limiting online processing. This paper presents an online loop-closure detection approach for large-scale and long- term operation. The approach is based on amemorymanagement method,which limits the number of locations used for loop-closure detection so that the computation time remains under real-time constraints. The idea consists of keeping the most recent and fre- quently observed locations in a working memory (WM) that is used for loop-closure detection, and transferring the others into a long-term memory (LTM). When a match is found between the current location and one stored in WM, associated locations that are stored in LTM can be updated and remembered for addi- tional loop-closure detections. Results demonstrate the approach's adaptability and scalability using ten standard datasets from other appearance-based loop-closure approaches, one custom dataset us- ing real images taken over a 2-km loop of our university campus, and one custom dataset (7 h) using virtual images from the racing video game “Need for Speed:MostWanted.”},
author = {Labbe, Mathieu and Michaud, Francois},
doi = {10.1109/TRO.2013.2242375},
file = {:home/jacob/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Labb{\'{e}}, Michaud - Unknown - Appearance-Based Loop Closure Detection for Online Large-Scale and Long-Term Operation.pdf:pdf},
isbn = {978-1-61284-455-8},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Appearance-based localization and mapping,bag-of-words approach,dynamic Bayes filtering,place recognition},
number = {3},
pages = {734--745},
pmid = {6459608},
title = {{Appearance-based loop closure detection for online large-scale and long-term operation}},
url = {https://introlab.3it.usherbrooke.ca/mediawiki-introlab/images/b/bc/TRO2013.pdf},
volume = {29},
year = {2013}
}
@inproceedings{Labbe2011a,
author = {Labbe, M. and Michaud, F.},
booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6094602},
file = {:home/jacob/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Labb{\'{e}}, Michaud - Unknown - Memory Management for Real-Time Appearance-Based Loop Closure Detection.pdf:pdf},
isbn = {978-1-61284-456-5},
month = {sep},
pages = {1271--1276},
publisher = {IEEE},
title = {{Memory management for real-time appearance-based loop closure detection}},
url = {http://ieeexplore.ieee.org/document/6094602/},
year = {2011}
}
@misc{Luotsinen2004,
author = {Luotsinen, Linus J and Gonzalez, Avelino J and Boeloeni, Ladislau},
file = {:home/jacob/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luotsinen, Gonzalez, Boeloeni - 2004 - Collaborative UAV Exploration of Hostile Environments.pdf:pdf},
keywords = {*ALGORITHMS,*AUTONOMOUS NAVIGATION,*COMBAT AREAS,*SURVEILLANCE DRONES,*TACTICAL DATA SYSTEMS,ADVERSE CONDITIONS,ARTIFICIAL INTELLIGENCE,COLLABORATIVE TECHNIQUES,COMPUTERIZED SIMULATION,DECISION MAKING,GRIDS(COORDINATES),INFORMATION TRANSFER,MAPS,SYMPOSIA},
title = {{Collaborative UAV Exploration of Hostile Environments}},
url = {http://www.dtic.mil/docs/citations/ADA432922},
year = {2004}
}
@article{Wheeler2017,
author = {Wheeler, David and Koch, Daniel and Jackson, James and Ellingson, Gary and Nyholm, Paul and McLain, Timothy and Beard, Randal},
journal = {All Faculty Publications},
month = {aug},
title = {{Relative Navigation of Autonomous GPS-Degraded Micro Air Vehicles}},
url = {https://scholarsarchive.byu.edu/facpub/1962},
year = {2017}
}
@article{Hornung2013,
author = {Hornung, Armin and Wurm, Kai M. and Bennewitz, Maren and Stachniss, Cyrill and Burgard, Wolfram},
doi = {10.1007/s10514-012-9321-0},
file = {:home/jacob/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornung et al. - 2013 - OctoMap an efficient probabilistic 3D mapping framework based on octrees.pdf:pdf},
issn = {0929-5593},
journal = {Autonomous Robots},
month = {apr},
number = {3},
pages = {189--206},
publisher = {Springer US},
title = {{OctoMap: an efficient probabilistic 3D mapping framework based on octrees}},
url = {http://link.springer.com/10.1007/s10514-012-9321-0},
volume = {34},
year = {2013}
}
